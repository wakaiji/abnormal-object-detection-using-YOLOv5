{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abnormal_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "s8d2yxkt4KPs",
        "k7-QGUM5nOl2",
        "PH7B4MNRFPSG",
        "c1pHA3_v0ioI",
        "IhJjFvQdbAoV",
        "4FAzPJQEaUE2",
        "WUHpzE0BRp_6",
        "ioSxsE4ZX0It",
        "DOMS87XTFHxc",
        "QwYMj6r0Tp0z",
        "diLCv0VxZvYa",
        "8A79FcLhaYa0",
        "6HLFko8Oa1x0",
        "JPjli1qxZWol",
        "6k1d9Sq7uXLe",
        "hF7fMP64umXF",
        "b6PcWDzwQPmB",
        "Nf_bhso0ujii",
        "68SOspzrBwaC",
        "nNzHjvwX6wEK"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZDo7-Z5kFIL"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAIVHNiT3bo0"
      },
      "source": [
        "!pip --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIwlUZmbx0ek"
      },
      "source": [
        "### **Connect to drive** ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GSSNZjKsvyD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yg7yCqoPoFV"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/yolov5/utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czBwi7SO6VC_"
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De791Y2VyIcC"
      },
      "source": [
        "### **Extract Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-QbIb5VxxgS"
      },
      "source": [
        "# !unzip -uq /content/gdrive/MyDrive/abnormal_detection/vinbigdata-chest-xray-abnormalities-detection \"train.csv\" -d \"/content/gdrive/MyDrive/abnormal_detection\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olPQKvSVyTQ9"
      },
      "source": [
        "### **Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDmnCiGqyfDY"
      },
      "source": [
        "!pip install pydicom\n",
        "!pip install iterative-stratification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ie-cAtplq_C"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import pydicom\n",
        "import cv2\n",
        "import PIL\n",
        "\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.offline as pyo\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "from os import path\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zT-5XJjF8WC"
      },
      "source": [
        "random_stat = 123\n",
        "np.random.seed(random_stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJXkThBE4XcG"
      },
      "source": [
        "DATA_DIR = '/content/gdrive/MyDrive/abnormal_detection'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMLex3jrGA8J"
      },
      "source": [
        "train_dcm_dir = os.path.join(DATA_DIR, \"train\")\n",
        "test_dcm_dir = os.path.join(DATA_DIR, \"test\")\n",
        "testing_dcm_dir = os.path.join(DATA_DIR, \"testing_images\")\n",
        "training_dcm_dir = os.path.join(DATA_DIR, \"train_image\")\n",
        "\n",
        "img_dir = os.path.join(DATA_DIR, 'images')\n",
        "label_dir = os.path.join(DATA_DIR, 'labels')\n",
        "metadata_dir = os.path.join(DATA_DIR, 'metadata')\n",
        "\n",
        "for directory in [img_dir, label_dir, metadata_dir]:\n",
        "  if os.path.isdir(directory):\n",
        "    continue\n",
        "  os.mkdir(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8d2yxkt4KPs"
      },
      "source": [
        "#### **Load CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp-q3QIE5WtL"
      },
      "source": [
        "annots = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
        "annots.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYU19RhnjQmI"
      },
      "source": [
        "annots.class_name.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suyp3jk_nCIY"
      },
      "source": [
        "size_data = pd.read_csv(\"/content/gdrive/MyDrive/abnormal_detection/train_meta.csv\")\n",
        "size_data.columns = ['image_id', 'h', 'w']\n",
        "\n",
        "annots = annots.merge(size_data, on='image_id', how='left')\n",
        "annots[['x_min', 'y_min']] = annots[['x_min', 'y_min']].fillna(0)\n",
        "annots[['x_max', 'y_max']] = annots[['x_max', 'y_max']].fillna(1)\n",
        "\n",
        "annots.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KoD5v8Uw1yt"
      },
      "source": [
        "data = annots[annots.class_id != 14].reset_index(drop = True)\n",
        "data.tail(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHs7sYDU3NVA"
      },
      "source": [
        "data.class_name.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcefJFzu4reF"
      },
      "source": [
        "data.class_name.value_counts().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3InkL3s_bz6T"
      },
      "source": [
        "### **Extract all patient data from DICOM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iplfOrS4b7k5"
      },
      "source": [
        "import pydicom as dicom\n",
        "import os\n",
        "import PIL # optional\n",
        "import pandas as pd\n",
        "import csv\n",
        "# list of attributes available in dicom image\n",
        "# download this file from the given github link\n",
        "dicom_image_description = pd.read_csv(\"dicom_image_description.csv\")\n",
        "# Specify the .dcm folder path\n",
        "folder_path = \"/content/gdrive/MyDrive/Pneumonia_dataset/testdicom\"\n",
        "images_path = os.listdir(folder_path)\n",
        "# Patient's information will be stored in working directory #'Patient_Detail.csv'\n",
        "with open('Patient_Detail.csv', 'w', newline ='') as csvfile:\n",
        "    fieldnames = list(dicom_image_description[\"Description\"])\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    writer.writerow(fieldnames)\n",
        "    for n, image in enumerate(images_path):\n",
        "        ds = dicom.dcmread(os.path.join(folder_path, image))\n",
        "        rows = []\n",
        "        for field in fieldnames:\n",
        "            if ds.data_element(field) is None:\n",
        "                rows.append('')\n",
        "            else:\n",
        "                x = str(ds.data_element(field)).replace(\"'\", \"\")\n",
        "                y = x.find(\":\")\n",
        "                x = x[y+2:]\n",
        "                rows.append(x)\n",
        "        writer.writerow(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3zTgRCwgDly"
      },
      "source": [
        "from pydicom.data import get_testdata_files\n",
        "from pydicom import dcmread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJbl2jXbfCjN"
      },
      "source": [
        "fpath = get_testdata_files(\"0a0b773c653cea6653a1e02faf1566a5.dicom\")\n",
        "ds = dcmread(fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpGd-zDjfceA"
      },
      "source": [
        "print(ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7-QGUM5nOl2"
      },
      "source": [
        "### **Visualize Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L83aKJz2nN6F"
      },
      "source": [
        "def Visualize_class(df, feature, title):\n",
        "  num_image = df[feature].value_counts().rename_axis(feature).reset_index(name=\"num_image\")\n",
        "  fig = px.bar(num_image[::-1], x='num_image', y=feature, orientation='h', color='num_image')\n",
        "  fig.update_layout(\n",
        "      title={\n",
        "          'text' : title,\n",
        "          'y' : 0.95,\n",
        "          'x' : 0.5,\n",
        "          'xanchor' : 'center',\n",
        "          'yanchor' : 'top'})\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbzgvRe3niwR"
      },
      "source": [
        "Visualize_class(annots, feature=\"class_name\", title=\"Types of abnormal labels\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8i0q3l0nsbY"
      },
      "source": [
        "Visualize_class(annots, feature=\"rad_id\", title=\"Types of radiologist\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKA-HefZZC4M"
      },
      "source": [
        "##**1. Data Migration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH7B4MNRFPSG"
      },
      "source": [
        "### **Generates Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgqrTOab0XDF"
      },
      "source": [
        "print(new_image_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_96UxC81yID"
      },
      "source": [
        "def save_img_from_dcm(dcm_dir, img_dir, image_id, voi_lut=True, fix_monochrome=True):\n",
        "  img_fp = os.path.join(img_dir, \"{}.png\".format(image_id))\n",
        "  if os.path.exists(img_fp):\n",
        "    return\n",
        "  dcm_fp = os.path.join(dcm_dir, \"{}.dicom\".format(image_id))\n",
        "  print(dcm_fp)\n",
        "  dicom = pydicom.read_file(dcm_fp)\n",
        "\n",
        "  if voi_lut:\n",
        "    data = apply_voi_lut(dicom.pixel_array, dicom)\n",
        "  else:\n",
        "    data = dicom.pixel_array\n",
        "\n",
        "  if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
        "    data = np.amax(data) - data\n",
        "  \n",
        "  data = data - np.min(data)\n",
        "  data = data / np.max(data)\n",
        "  data = (data * 255).astype(np.uint8)\n",
        "\n",
        "  img_fp = os.path.join(img_dir, \"{}.png\".format(image_id))\n",
        "  cv2.imwrite(img_fp, data)\n",
        "\n",
        "  \n",
        "def save_img_from_abnormal(dcm_dir, img_dir, annots):\n",
        "  for row in tqdm(annots.image_id.unique()):\n",
        "    image_id = row\n",
        "\n",
        "    img_fp = os.path.join(img_dir, \"{}.png\".format(image_id))\n",
        "    if os.path.exists(img_fp):\n",
        "      continue\n",
        "    \n",
        "    save_img_from_dcm(dcm_dir, img_dir, image_id)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1whRGjRCYNdz"
      },
      "source": [
        "save_img_from_abnormal(train_dcm_dir, img_dir, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1pHA3_v0ioI"
      },
      "source": [
        "### **Generates Images Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-0GP-Fj0OrH"
      },
      "source": [
        "new_image_id = []\n",
        "for filename in tqdm(os.listdir(\"/content/gdrive/MyDrive/abnormal_detection/test\")):\n",
        "  file = filename.split('.')\n",
        "  if file[-1] == \"dicom\":\n",
        "     new_image_id.append(file[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuVxkLXo0wUy"
      },
      "source": [
        "def save_img_from_testing(dcm_dir, img_dir, annots):\n",
        "  for row in tqdm(annots):\n",
        "    image_id = row\n",
        "\n",
        "    img_fp = os.path.join(img_dir, \"{}.jpg\".format(image_id))\n",
        "    if os.path.exists(img_fp):\n",
        "      continue\n",
        "    \n",
        "    save_img_from_dcm(dcm_dir, img_dir, image_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Atg0Xi_0oHI"
      },
      "source": [
        "save_img_from_testing(test_dcm_dir, img_dir, new_image_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhJjFvQdbAoV"
      },
      "source": [
        "###**Generating Labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T2ZoLm6e5GK"
      },
      "source": [
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from bokeh.plotting import figure as bokeh_figure\n",
        "from bokeh.io import output_notebook, show, output_file\n",
        "from bokeh.models import ColumnDataSource, HoverTool, Panel\n",
        "from bokeh.models.widgets import Tabs\n",
        "import random\n",
        "from random import randint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa_7880bdz1C"
      },
      "source": [
        "def get_bbox_area(row):\n",
        "  return (row['x_max']-row['x_min'])*(row['y_max']-row['y_min'])\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "annots['rad_label'] = le.fit_transform(annots['rad_id'])\n",
        "\n",
        "finding_df = annots[(annots['class_name'] != 'No finding')]\n",
        "finding_df['bbox_area'] = finding_df.apply(get_bbox_area, axis=1)\n",
        "finding_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycFQMtagf3FW"
      },
      "source": [
        "def dicom2array(path, voi_lut=True, fix_monochrome=True):\n",
        "    dicom = pydicom.read_file(path)\n",
        "    # VOI LUT (if available by DICOM device) is used to\n",
        "    # transform raw DICOM data to \"human-friendly\" view\n",
        "    if voi_lut:\n",
        "        data = apply_voi_lut(dicom.pixel_array, dicom)\n",
        "    else:\n",
        "        data = dicom.pixel_array\n",
        "    # depending on this value, X-ray may look inverted - fix that:\n",
        "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
        "        data = np.amax(data) - data\n",
        "    data = data - np.min(data)\n",
        "    data = data / np.max(data)\n",
        "    data = (data * 255).astype(np.uint8)\n",
        "    return data\n",
        "  \n",
        "def plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(500,500)):\n",
        "  rows = len(imgs)//cols + 1\n",
        "  fig = plt.figure(figsize=(cols*size, rows*size))\n",
        "  for i, img in enumerate(imgs):\n",
        "      if img_size is not None:\n",
        "          img = cv2.resize(img, img_size)\n",
        "      fig.add_subplot(rows, cols, i+1)\n",
        "      plt.imshow(img, cmap=cmap)\n",
        "  plt.suptitle(title)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U3Pgi7ufF7-"
      },
      "source": [
        "imgs = []\n",
        "img_ids = finding_df['image_id'].values\n",
        "class_ids = finding_df['class_id'].unique()\n",
        "\n",
        "# map label_id to specify color\n",
        "label2color = {class_id:[randint(0,255) for i in range(3)] for class_id in class_ids}\n",
        "thickness = 3\n",
        "scale = 5\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    img_id = random.choice(img_ids)\n",
        "    img_path = os.path.join(train_dcm_dir, \"{}.dicom\".format(img_id))\n",
        "    img = dicom2array(path=img_path)\n",
        "    img = cv2.resize(img, None, fx=1/scale, fy=1/scale)\n",
        "    img = np.stack([img, img, img], axis=-1)\n",
        "    \n",
        "    boxes = finding_df.loc[finding_df['image_id'] == img_id, ['x_min', 'y_min', 'x_max', 'y_max']].values/scale\n",
        "    labels = finding_df.loc[finding_df['image_id'] == img_id, ['class_id']].values.squeeze()\n",
        "    \n",
        "    for label_id, box in zip(labels, boxes):\n",
        "      color = label2color[label_id]\n",
        "      img = cv2.rectangle(\n",
        "          img,\n",
        "          (int(box[0]), int(box[1])),\n",
        "          (int(box[2]), int(box[3])),\n",
        "          color, thickness)\n",
        "\n",
        "    img = cv2.resize(img, (500,500))\n",
        "    imgs.append(img)\n",
        "    \n",
        "plot_imgs(imgs, cmap=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGvkIWtjNIhn"
      },
      "source": [
        "testing_train = pd.read_csv('/content/testing_train.csv')\n",
        "testing_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F56w0IKYZEh"
      },
      "source": [
        "files = []\n",
        "for filename in tqdm(os.listdir(\"/content/gdrive/MyDrive/abnormal_detection/testing_images\")):\n",
        "  file = filename.split(\".\")\n",
        "  files.append(file[0])\n",
        "\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "508PlShoHiik"
      },
      "source": [
        "def save_label_from_abnormal(testing_dir, annots):\n",
        "  for row in tqdm(annots.values):\n",
        "    image_id = row[0]\n",
        "\n",
        "    label_dir = \"/content/gdrive/MyDrive/abnormal_detection/images\"\n",
        "\n",
        "    image = PIL.Image.open(os.path.join(label_dir, \"{}.jpg\".format(image_id)))\n",
        "    width, height = image.size\n",
        "    label_fp = os.path.join(label_dir, \"{}.txt\".format(image_id))\n",
        "\n",
        "    f = open(label_fp, \"a\")\n",
        "    if row[2] is 14:\n",
        "      x_min = 0\n",
        "      y_min = 0\n",
        "      x_max = 1\n",
        "      y_max = 1\n",
        "    else:\n",
        "      x_min = row[4]\n",
        "      y_min = row[5]\n",
        "      x_max = row[6]\n",
        "      y_max = row[7]\n",
        "\n",
        "    dw = 1./width\n",
        "    dh = 1./height\n",
        "    x = (x_min + x_max)/2.0\n",
        "    y = (y_min + y_max)/2.0\n",
        "    w = x_max - x_min\n",
        "    h = y_max - y_min\n",
        "\n",
        "    x = x*dw\n",
        "    w = w*dw\n",
        "    y = y*dh\n",
        "    h = h*dh\n",
        "\n",
        "    line = \"{} {} {} {} {}\\n\".format(row[2], x, y, w, h)      \n",
        "\n",
        "    f.write(line)\n",
        "    f.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acKpG6gqcwk1"
      },
      "source": [
        "save_label_from_abnormal(img_dir, annots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAzPJQEaUE2"
      },
      "source": [
        "## **2. Data Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUHpzE0BRp_6"
      },
      "source": [
        "### **Weighted Box Fusion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_x9ESIpSHbL"
      },
      "source": [
        "!pip install ensemble_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT94HKQLSFDJ"
      },
      "source": [
        "from ensemble_boxes import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7H64qTNmRX_"
      },
      "source": [
        "IMG_SIZE = (512, 512)\n",
        "list_remove = [34843, 21125, 647, 18011, 2539, 22373, 12675, 7359, 20642, 5502, 19818, 5832, 28056, 28333, 20758,\n",
        "               925, 43, 2199, 4610, 21306, 16677, 1768, 17232, 1378, 24949, 30203, 31410, 87, 25318, 92, 31724,\n",
        "               118, 17687, 12605, 26157, 33875, 7000, 3730, 18776, 13225, 1109, 2161, 33627, 15500, 28633, 28152,\n",
        "               10114, 10912, 9014,  4427, 25630, 11464, 6419, 22164, 4386, 17557, 15264, 21853, 33142, 32895, 9733,\n",
        "               33010, 17493, 32128, 28802, 11658, 8841, 29557, 4802, 8591, 778, 9935, 12359, 5210, 7556, 24505, 5664,\n",
        "               28670, 27820, 19359, 9817, 7800, 32934, 34098, 27931, 16074, 27308, 30645, 31029, 35697, 6199, 27065,\n",
        "               1771, 14689, 31860, 1975, 29294, 2304, 34018, 23406, 26501, 26011, 2479, 32796, 25836, 3032, 31454,\n",
        "               32066, 19722, 15997, 6049, 9458, 11005, 23151, 24503, 35411, 18092, 23815, 30742, 33942, 34542, 7655,\n",
        "               25345, 3750, 17046, 3844, 5958, 4250, 18823, 14898, 22581, 25805, 9651, 33194, 36007, 30160, 24459,\n",
        "               10838, 16544, 31252, 8053, 28487, 6208, 25244, 8470, 10089, 24813, 14769, 34305, 34047, 23366, 8049,\n",
        "               13276, 22380, 32797, 32440, 11031, 18304, 33692, 21349, 26333, 34331, 9110, 21092, 34882, 35626, 10203,\n",
        "               25648, 30754, 29567, 33542, 15146, 26759, 20846, 22493, 33187, 22813, 30219, 14548, 14627, 20494, 28332,\n",
        "               15930, 31347, 33489, 35005, 34032, 24183, 18643, 18536, 29754, 20380, 29750, 20539, 35791, 27275, 32248]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZ9ZPxpkJrb"
      },
      "source": [
        "def label_resize(org_size, img_size, *bbox):\n",
        "    x0, y0, x1, y1 = bbox\n",
        "    x0_new = int(np.round(x0*img_size[1]/org_size[1]))\n",
        "    y0_new = int(np.round(y0*img_size[0]/org_size[0]))\n",
        "    x1_new = int(np.round(x1*img_size[1]/org_size[1]))\n",
        "    y1_new = int(np.round(y1*img_size[0]/org_size[0]))\n",
        "    return x0_new, y0_new, x1_new, y1_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7GxUFPTcBuu"
      },
      "source": [
        "train_abnormal = annots[annots['class_name'] != 'No finding'].reset_index(drop=True)\n",
        "train_abnormal[['x_min_resize', 'y_min_resize', 'x_max_resize', 'y_max_resize']] = train_abnormal.apply(lambda x: label_resize(x[['h','w']].values, IMG_SIZE, *x[['x_min', 'y_min', 'x_max', 'y_max']].values), axis=1, result_type=\"expand\")\n",
        "train_abnormal[['x_min', 'y_min', 'x_max', 'y_max']] = train_abnormal[['x_min', 'y_min', 'x_max', 'y_max']]\n",
        "train_abnormal['x_center'] = 0.5*(train_abnormal['x_min_resize'] + train_abnormal['x_max_resize'])\n",
        "train_abnormal['y_center'] = 0.5*(train_abnormal['y_min_resize'] + train_abnormal['y_max_resize'])\n",
        "train_abnormal['width'] = train_abnormal['x_max_resize'] - train_abnormal['x_min_resize']\n",
        "train_abnormal['height'] = train_abnormal['y_max_resize'] - train_abnormal['y_min_resize']\n",
        "train_abnormal['area'] = train_abnormal.apply(lambda x: (x['x_max_resize'] - x['x_min_resize'])*(x['y_max_resize']-x['y_min_resize']), axis=1)\n",
        "train_abnormal = train_abnormal[~train_abnormal.index.isin(list_remove)].reset_index(drop=True)\n",
        "\n",
        "train_abnormal.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OysneHCERyoz"
      },
      "source": [
        "SIZE = 512\n",
        "def Preprocess_wbf(df, size=SIZE, iou_thr=0.5, skip_box_thr=0.0001):\n",
        "    list_image = []\n",
        "    list_boxes = []\n",
        "    list_cls = []\n",
        "    list_h, list_w = [], []\n",
        "    new_df = pd.DataFrame()\n",
        "    for image_id in tqdm(df['image_id'].unique(), leave=False):\n",
        "        image_df = df[df['image_id']==image_id].reset_index(drop=True)\n",
        "        h, w = image_df.loc[0, ['h', 'w']].values\n",
        "        boxes = image_df[['x_min_resize', 'y_min_resize',\n",
        "                          'x_max_resize', 'y_max_resize']].values.tolist()\n",
        "        boxes = [[j/(size-1) for j in i] for i in boxes]\n",
        "        scores = [1.0]*len(boxes)\n",
        "        labels = [float(i) for i in image_df['class_id'].values]\n",
        "        boxes, scores, labels = weighted_boxes_fusion([boxes], [scores], [labels],\n",
        "                                                      weights=None,\n",
        "                                                      iou_thr=iou_thr,\n",
        "                                                      skip_box_thr=skip_box_thr)\n",
        "        list_image.extend([image_id]*len(boxes))\n",
        "        list_h.extend([h]*len(boxes))\n",
        "        list_w.extend([w]*len(boxes))\n",
        "        list_boxes.extend(boxes)\n",
        "        list_cls.extend(labels.tolist())\n",
        "    list_boxes = [[int(j*(size-1)) for j in i] for i in list_boxes]\n",
        "    new_df['image_id'] = list_image\n",
        "    new_df['class_id'] = list_cls\n",
        "    new_df['h'] = list_h\n",
        "    new_df['w'] = list_w\n",
        "    new_df['x_min_resize'], new_df['y_min_resize'], new_df['x_max_resize'], new_df['y_max_resize'] = np.transpose(list_boxes)\n",
        "    new_df['x_center'] = 0.5*(new_df['x_min_resize'] + new_df['x_max_resize'])\n",
        "    new_df['y_center'] = 0.5*(new_df['y_min_resize'] + new_df['y_max_resize'])\n",
        "    new_df['width'] = new_df['x_max_resize'] - new_df['x_min_resize']\n",
        "    new_df['height'] = new_df['y_max_resize'] - new_df['y_min_resize']\n",
        "    new_df['area'] = new_df.apply(lambda x: (x['x_max_resize']-x['x_min_resize'])*(x['y_max_resize']-x['y_min_resize']), axis=1)\n",
        "    return new_df\n",
        "\n",
        "new_train_abnormal = Preprocess_wbf(train_abnormal)\n",
        "new_train_abnormal.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOFDPn3obBpA"
      },
      "source": [
        "new_train_abnormal.to_csv('abnormal.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zimsSz7cmq_2"
      },
      "source": [
        "### **Fold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzZ43I3QmqVD"
      },
      "source": [
        "def split_df(df):\n",
        "  kf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=89)\n",
        "  df['id'] = df.index\n",
        "  annot_pivot = pd.pivot_table(df, index=['image_id'], columns=['class_id'], values='id', fill_value=0, aggfunc='count').reset_index().rename_axis(None, axis=1)\n",
        "  for fold, (train_idx, val_idx) in enumerate(kf.split(annot_pivot, annot_pivot.iloc[:, 1:(1+df['class_id'].nunique())])):\n",
        "    annot_pivot[f'fold_{fold}'] = 0\n",
        "    annot_pivot.loc[val_idx, f'fold_{fold}'] = 1\n",
        "  return annot_pivot\n",
        "\n",
        "size_df = pd.read_csv('/content/gdrive/MyDrive/abnormal_detection/train_meta.csv')\n",
        "size_df.columns = ['image_id', 'h', 'w']\n",
        "\n",
        "fold_csv = split_df(new_train_abnormal)\n",
        "fold_csv = fold_csv.merge(size_df, on='image_id', how='left')\n",
        "fold_csv.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioSxsE4ZX0It"
      },
      "source": [
        "### **Drop \"No finding\" and move data to new folder for train images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaLoIY6W-CbI"
      },
      "source": [
        "new_dataset = \"/content/gdrive/MyDrive/abnormal_detection/dataset_baru\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTepWKlDBpxN"
      },
      "source": [
        "def write_images_list(target_dir, img_dir, series):\n",
        "  for image_id in series:\n",
        "    images_dir = os.path.join(img_dir, '{}.jpg'.format(image_id))\n",
        "    \n",
        "    shutil.copyfile(images_dir, os.path.join(target_dir, '{}.jpg'.format(image_id)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP00vb0e9h--"
      },
      "source": [
        "id_series = new_train_abnormal[\"image_id\"].drop_duplicates()\n",
        "write_images_list(new_dataset, img_dir, id_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOMS87XTFHxc"
      },
      "source": [
        "### **Generate Label**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IhamPvjFSGu"
      },
      "source": [
        "def save_label_from_abnormal(target_dir, annots):\n",
        "  SIZE = 512\n",
        "  for _, row in tqdm(annots.iterrows()):\n",
        "    image_id = row['image_id']\n",
        "\n",
        "    label_fp = os.path.join(target_dir, \"{}.txt\".format(image_id))\n",
        "\n",
        "    f = open(label_fp, \"a\")\n",
        "\n",
        "    dw = 1./SIZE\n",
        "    dh = 1./SIZE\n",
        "    x_center = row['x_center']\n",
        "    y_center = row['y_center']\n",
        "    width = row['width']\n",
        "    height = row['height']\n",
        "\n",
        "    x_center = x_center*dw\n",
        "    width = width*dw\n",
        "    y_center = y_center*dh\n",
        "    height = height*dh\n",
        "\n",
        "    line = \"{} {} {} {} {}\\n\".format(int(row['class_id']), x_center, y_center, width, height)      \n",
        "\n",
        "    f.write(line)\n",
        "    f.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS91rbmYFSGu"
      },
      "source": [
        "dataset_path = \"/content/gdrive/MyDrive/abnormal_detection/images\"\n",
        "save_label_from_abnormal(dataset_path, new_train_abnormal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwYMj6r0Tp0z"
      },
      "source": [
        "## **3. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diLCv0VxZvYa"
      },
      "source": [
        "### **Resize 640**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWDuSmdUToNi"
      },
      "source": [
        "def resize_640(directory):\n",
        "  for filename in tqdm(os.listdir(directory)):\n",
        "    file = filename.split('.')\n",
        "    if file[-1] == 'png':\n",
        "      image_name = os.path.join(directory, \"{}.png\".format(file[0]))\n",
        "      img = cv2.imread(image_name, 0)\n",
        "\n",
        "      scale_percent = 40\n",
        "\n",
        "      width = int(img.shape[1] * scale_percent/100)\n",
        "      height = int(img.shape[0] * scale_percent/100)\n",
        "\n",
        "      dsize = (width, height)\n",
        "\n",
        "      output = cv2.resize(img, dsize)\n",
        "\n",
        "      cv2.imwrite(image_name, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzYotKifxPML"
      },
      "source": [
        "resize_640('/content/gdrive/MyDrive/abnormal_detection/training_7/images/val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A79FcLhaYa0"
      },
      "source": [
        "## **4. Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK4baWAfv8ss"
      },
      "source": [
        "def brightness_augment(img, factor=0.5): \n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) #convert to hsv\n",
        "    hsv = np.array(hsv, dtype=np.float64)\n",
        "    hsv[:, :, 2] = hsv[:, :, 2] * (factor + np.random.uniform()) #scale channel V uniformly\n",
        "    hsv[:, :, 2][hsv[:, :, 2] > 255] = 255 #reset out of range values\n",
        "    rgb = cv2.cvtColor(np.array(hsv, dtype=np.uint8), cv2.COLOR_HSV2RGB)\n",
        "    return rgb\n",
        "\n",
        "\n",
        "img_id = \"ffeffc54594debf3716d6fcd2402a99f\"\n",
        "folder_path = \"/content/gdrive/MyDrive/abnormal_detection/testing_images\"\n",
        "\n",
        "img_path = os.path.join(folder_path, \"{}.jpg\".format(img_id))\n",
        "bbox_path = os.path.join(folder_path, \"{}.txt\".format(img_id))\n",
        "\n",
        "f = open(bbox_path, 'r')\n",
        "f1 = f.readlines()\n",
        "print(f1)\n",
        "\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "(height, width) = img.shape[:2]\n",
        "\n",
        "quarter_height, quarter_width = height/4, width/4\n",
        "\n",
        "M = np.float32([[1, 0, quarter_height], [0, 1, quarter_width]])\n",
        "\n",
        "# Translation\n",
        "# img = cv2.warpAffine(img, M, (width, height))\n",
        "\n",
        "# Scaling\n",
        "# img = cv2.resize(img, (int(width / 2), int(height / 2)), interpolation = cv2.INTER_CUBIC)\n",
        "# img = cv2.resize(img, None, fx=2, fy=2, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "# Random Brightness\n",
        "# equ = cv2.equalizeHist(img)\n",
        "ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n",
        "channels = cv2.split(ycrcb)\n",
        "# cv2.equalizeHist(channels[0], channels[0])\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "clahe.apply(channels[0], channels[0])\n",
        "cv2.merge(channels, ycrcb)\n",
        "cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, img)\n",
        "# clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "# equ = clahe.apply(img)\n",
        "\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0TWZDvAdnU7"
      },
      "source": [
        "img_path = os.path.join(img_dir, \"{}.png\".format(\"0b4c3cb187f7582fa482b8f0fded6bd7\"))\n",
        "txt_path = os.path.join(img_dir, \"{}.txt\".format(\"0b4c3cb187f7582fa482b8f0fded6bd7\"))\n",
        "\n",
        "img = cv2.imread(img_path,1)\n",
        "H, W = img.shape[:2]\n",
        "\n",
        "f = open(txt_path, 'r')\n",
        "f1 = f.readlines()\n",
        "new_bbox = []\n",
        "for x in f1:\n",
        "  bbox = x.strip('\\n').split(' ')\n",
        "  if len(bbox) > 1:\n",
        "    (center_x, center_y, bbox_width, bbox_height) = yoloFormattocv(float(bbox[1]), float(bbox[2]), float(bbox[3]), float(bbox[4]), H, W)\n",
        "    new_bbox.append([bbox[0], center_x, center_y, bbox_width, bbox_height])\n",
        "\n",
        "class_ids = data['class_id'].unique()\n",
        "\n",
        "# map label_id to specify color\n",
        "label2color = {class_id:[randint(0,255) for i in range(3)] for class_id in class_ids}\n",
        "thickness = 15\n",
        "scale = 5\n",
        "\n",
        "for box in new_bbox:\n",
        "  color = label2color[int(box[0])]\n",
        "  img = cv2.rectangle(\n",
        "      img,\n",
        "      (int(box[1]), int(box[2])),\n",
        "      (int(box[3]), int(box[4])),\n",
        "      color, thickness)\n",
        "\n",
        "img = cv2.resize(img, (W,H))\n",
        "    \n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HLFko8Oa1x0"
      },
      "source": [
        "### **Function yoloFormattocv and cvFormattoYolo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z2l0u-VaXbC"
      },
      "source": [
        "#convert from Yolo_mark to opencv format\n",
        "def yoloFormattocv(x1, y1, x2, y2, H, W):\n",
        "    bbox_width = x2 * W\n",
        "    bbox_height = y2 * H\n",
        "    center_x = x1 * W\n",
        "    center_y = y1 * H\n",
        "    voc = []\n",
        "    voc.append(center_x - (bbox_width / 2))\n",
        "    voc.append(center_y - (bbox_height / 2))\n",
        "    voc.append(center_x + (bbox_width / 2))\n",
        "    voc.append(center_y + (bbox_height / 2))\n",
        "    return [int(v) for v in voc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L7I9ZSJyESv"
      },
      "source": [
        "# Convert from opencv format to yolo format\n",
        "# H,W is the image height and width\n",
        "def cvFormattoYolo(corner, H, W):\n",
        "    bbox_W = corner[3] - corner[1]\n",
        "    bbox_H = corner[4] - corner[2]\n",
        "    center_bbox_x = (corner[1] + corner[3]) / 2\n",
        "    center_bbox_y = (corner[2] + corner[4]) / 2\n",
        "    return corner[0], round(center_bbox_x / W, 6), round(center_bbox_y / H, 6), round(bbox_W / W, 6),round(bbox_H / H, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPjli1qxZWol"
      },
      "source": [
        "### **Rotate 20 and 340 degree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPn2PgjUyypg"
      },
      "source": [
        "class yoloRotatebbox:\n",
        "    def __init__(self, filename, label_name, image_ext, angle):\n",
        "        assert os.path.isfile(filename + image_ext)\n",
        "        assert os.path.isfile(label_name)\n",
        "        \n",
        "        self.filename = filename\n",
        "        self.label_name = label_name\n",
        "        self.image_ext = image_ext\n",
        "        self.angle = angle\n",
        "        \n",
        "        # Read image using cv2\n",
        "        self.image = cv2.imread(self.filename + self.image_ext, 1)\n",
        "        \n",
        "        rotation_angle = self.angle * np.pi / 180\n",
        "        self.rot_matrix = np.array([[np.cos(rotation_angle), -np.sin(rotation_angle)], [np.sin(rotation_angle), np.cos(rotation_angle)]])\n",
        "        \n",
        "    def rotateYolobbox(self):\n",
        "        new_height, new_width = self.rotate_image().shape[:2]\n",
        "        f = open(self.label_name, 'r')\n",
        "        f1 = f.readlines()\n",
        "        new_bbox = []\n",
        "        H, W = self.image.shape[:2]\n",
        "        for x in f1:\n",
        "            bbox = x.strip('\\n').split(' ')\n",
        "            if len(bbox) > 1:\n",
        "                (center_x, center_y, bbox_width, bbox_height) = yoloFormattocv(float(bbox[1]), float(bbox[2]), float(bbox[3]), float(bbox[4]), H, W)\n",
        "                upper_left_corner_shift = (center_x - W / 2, -H / 2 + center_y)\n",
        "                upper_right_corner_shift = (bbox_width - W / 2, -H / 2 + center_y)\n",
        "                lower_left_corner_shift = (center_x - W / 2, -H / 2 + bbox_height)\n",
        "                lower_right_corner_shift = (bbox_width - W / 2, -H / 2 + bbox_height)\n",
        "                new_lower_right_corner = [-1, -1]\n",
        "                new_upper_left_corner = []\n",
        "                for i in (upper_left_corner_shift, upper_right_corner_shift, lower_left_corner_shift,\n",
        "                          lower_right_corner_shift):\n",
        "                    new_coords = np.matmul(self.rot_matrix, np.array((i[0], -i[1])))\n",
        "                    x_prime, y_prime = new_width / 2 + new_coords[0], new_height / 2 - new_coords[1]\n",
        "                    if new_lower_right_corner[0] < x_prime:\n",
        "                        new_lower_right_corner[0] = x_prime\n",
        "                    if new_lower_right_corner[1] < y_prime:\n",
        "                        new_lower_right_corner[1] = y_prime\n",
        "                    if len(new_upper_left_corner) > 0:\n",
        "                        if new_upper_left_corner[0] > x_prime:\n",
        "                            new_upper_left_corner[0] = x_prime\n",
        "                        if new_upper_left_corner[1] > y_prime:\n",
        "                            new_upper_left_corner[1] = y_prime\n",
        "                    else:\n",
        "                        new_upper_left_corner.append(x_prime)\n",
        "                        new_upper_left_corner.append(y_prime)\n",
        "                #             print(x_prime, y_prime)\n",
        "\n",
        "                new_bbox.append([bbox[0], new_upper_left_corner[0], new_upper_left_corner[1],\n",
        "                                 new_lower_right_corner[0], new_lower_right_corner[1]])\n",
        "        return new_bbox\n",
        "        \n",
        "    def rotate_image(self):\n",
        "        \"\"\"\n",
        "        Rotates an image (angle in degrees) and expands image to avoid cropping\n",
        "        \"\"\"\n",
        "        height, width = self.image.shape[:2]  # image shape has 3 dimensions\n",
        "        image_center = (width / 2, height / 2)  # getRotationMatrix2D needs coordinates in reverse order (width, height) compared to shape\n",
        "        rotation_mat = cv2.getRotationMatrix2D(image_center, self.angle, 1.)\n",
        "        # rotation calculates the cos and sin, taking absolutes of those.\n",
        "        abs_cos = abs(rotation_mat[0, 0])\n",
        "        abs_sin = abs(rotation_mat[0, 1])\n",
        "        # find the new width and height bounds\n",
        "        bound_w = int(height * abs_sin + width * abs_cos)\n",
        "        bound_h = int(height * abs_cos + width * abs_sin)\n",
        "        # subtract old image center (bringing image back to origin) and adding the new image center coordinates\n",
        "        rotation_mat[0, 2] += bound_w / 2 - image_center[0]\n",
        "        rotation_mat[1, 2] += bound_h / 2 - image_center[1]\n",
        "        # rotate image with the new bounds and translated rotation matrix\n",
        "        rotated_mat = cv2.warpAffine(self.image, rotation_mat, (bound_w, bound_h))\n",
        "        return rotated_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNV2RcE76di0"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    angels=180\n",
        "    image_id_translation = tr_series\n",
        "    img_path = \"/content/gdrive/MyDrive/abnormal_detection/training_7/images/train\"\n",
        "    label_path = \"/content/gdrive/MyDrive/abnormal_detection/training_7/labels/train\"\n",
        "    index = 0\n",
        "    # \n",
        "    for image_id in tqdm(tr_series):\n",
        "      # split_image = image_ids.split('.')\n",
        "      # image_id = split_image[0]\n",
        "      # image_ext = \".\"+split_image[1]\n",
        "      # if index > 1:\n",
        "      #   index = 0\n",
        "\n",
        "      image_ext = \".png\"\n",
        "\n",
        "      image_name = os.path.join(img_path, image_id)\n",
        "      label_name = os.path.join(label_path, image_id)\n",
        "      print(image_id)\n",
        "      if not os.path.exists(image_name+image_ext):\n",
        "        continue\n",
        "      im = yoloRotatebbox(image_name, label_name+'.txt', image_ext, angels)\n",
        "      bbox = im.rotateYolobbox()\n",
        "      image = im.rotate_image()\n",
        "      # to write rotateed image to disk\n",
        "      cv2.imwrite(image_name+'_' + str(angels) + '.png', image)\n",
        "      file_name = label_name+'_' + str(angels) + '.txt'\n",
        "\n",
        "      #print(\"For angle \"+str(angle))\n",
        "      if os.path.exists(file_name):\n",
        "          os.remove(file_name)\n",
        "      # to write the new rotated bboxes to file\n",
        "      for i in bbox:\n",
        "          with open(file_name, 'a') as fout:\n",
        "            fout.writelines(' '.join(map(str, cvFormattoYolo(i, im.rotate_image().shape[0], im.rotate_image().shape[1]))) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbiuu3YyxZun"
      },
      "source": [
        "image_id_translation[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k1d9Sq7uXLe"
      },
      "source": [
        "### **Translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47156HPCBj-k"
      },
      "source": [
        "class yoloShiftbbox:\n",
        "  def __init__(self, filename, label_name, image_ext, shift):\n",
        "    assert os.path.isfile(filename + image_ext)\n",
        "    assert os.path.isfile(label_name)\n",
        "\n",
        "    self.filename = filename\n",
        "    self.label_name = label_name\n",
        "    self.image_ext = image_ext\n",
        "\n",
        "    # Read image using cv2\n",
        "    self.image = cv2.imread(self.filename + self.image_ext, 1)\n",
        "\n",
        "    self.shift = shift\n",
        "    self.shape_of_out_img = self.image.shape\n",
        "\n",
        "\n",
        "  def shiftYolobbox(self):\n",
        "    x_distance = self.shift[0]\n",
        "    y_distance = self.shift[1]\n",
        "\n",
        "    f = open(self.label_name , 'r')\n",
        "    f1 = f.readlines()\n",
        "    new_bbox = []\n",
        "    H, W = self.image.shape[:2]\n",
        "    for x in f1:\n",
        "      bbox = x.strip('\\n').split(' ')\n",
        "      if len(bbox) > 1:\n",
        "        (center_x, center_y, bbox_width, bbox_height) = yoloFormattocv(float(bbox[1]), float(bbox[2]), float(bbox[3]), float(bbox[4]), H, W)\n",
        "        # bbox scale formula\n",
        "        x1 = center_x + x_distance\n",
        "        y1 = center_y + y_distance\n",
        "        x2 = bbox_width + x_distance\n",
        "        y2 = bbox_height + y_distance\n",
        "      \n",
        "      # (objek, bbox_x_1, bbox_y_1, bbox_x_2, bbox_x_2) = cvFormattoYolo([bbox[0], x1, y1, x2, y2], H, W)\n",
        "      # if (bbox_x_1 < 0 or bbox_y_1 < 0 or bbox_x_2 < 0 or bbox_x_2 < 0):\n",
        "      #   continue\n",
        "\n",
        "      new_bbox.append([bbox[0], x1, y1, x2, y2])\n",
        "\n",
        "    return new_bbox\n",
        "\n",
        "  def translation_image(self):\n",
        "    h,w = self.image.shape[:2]\n",
        "    x_distance = self.shift[0]\n",
        "    y_distance = self.shift[1]\n",
        "    ts_mat = np.float32([[1,0,x_distance], [0,1,y_distance]])\n",
        "\n",
        "    out_img = np.zeros(self.shape_of_out_img, dtype='u1')\n",
        "    out_img = cv2.warpAffine(self.image, ts_mat, (w, h))\n",
        "\n",
        "    # for i in range(h):\n",
        "    #   for j in range(w):\n",
        "    #     origin_x = j\n",
        "    #     origin_y = i\n",
        "    #     origin_xy = np.array([origin_x, origin_y, 1])\n",
        "\n",
        "    #     new_xy = np.dot(ts_mat, origin_xy)\n",
        "    #     new_x = new_xy[0]\n",
        "    #     new_y = new_xy[1]\n",
        "\n",
        "    #     if 0 < new_x < w and 0 < new_y < h:\n",
        "    #       out_img[new_y, new_x] = self.image[i, j]\n",
        "    return out_img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAs81hPPxLEh"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  image_id_translation = tr_series\n",
        "  img_path = \"/content/gdrive/MyDrive/abnormal_detection/training_7/images/train\"\n",
        "  label_path = \"/content/gdrive/MyDrive/abnormal_detection/training_7/labels/train\"\n",
        "  shift = [[-500, 500],[300, 400],[-300, 400]]\n",
        "  i_for_shift = 0\n",
        "  # \n",
        "  for image_id in tqdm(image_id_translation):\n",
        "    if i_for_shift > 2:\n",
        "      i_for_shift = 0\n",
        "\n",
        "    image_ext = \".png\"\n",
        "    image_name = os.path.join(img_path, image_id)\n",
        "    label_name = os.path.join(label_path, image_id)\n",
        "\n",
        "    if not os.path.exists(image_name+image_ext):\n",
        "      continue\n",
        "\n",
        "    im = yoloShiftbbox(image_name, label_name+\".txt\", image_ext, shift[i_for_shift])\n",
        "    bbox = im.shiftYolobbox()\n",
        "    image = im.translation_image()\n",
        "    # to write rotateed image to disk\n",
        "    cv2.imwrite(image_name+'_' + \"translation\" + '.png', image)\n",
        "    file_name = label_name+'_' + \"translation\" + '.txt'\n",
        "    #print(\"For angle \"+str(angle))\n",
        "    i_for_shift+=1\n",
        "    if os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "    # to write the new rotated bboxes to file\n",
        "    for i in bbox:\n",
        "        with open(file_name, 'a') as fout:\n",
        "          fout.writelines(' '.join(map(str, cvFormattoYolo(i, im.translation_image().shape[0], im.translation_image().shape[1]))) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF7fMP64umXF"
      },
      "source": [
        "### **Histogram Equalization & CLAHE (Contrast Limited Adaptive Histogram Equalization)** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmy2tz_PiCK8"
      },
      "source": [
        "class yoloHistogramEqualization:\n",
        "  def __init__(self, filename, label_name, image_ext):\n",
        "    assert os.path.isfile(filename + image_ext)\n",
        "    assert os.path.isfile(label_name + '.txt')\n",
        "\n",
        "    self.filename = filename\n",
        "    self.label_name = label_name\n",
        "    self.image_ext = image_ext\n",
        "\n",
        "    # Read image using cv2\n",
        "    self.image = cv2.imread(self.filename + self.image_ext, 1)\n",
        "\n",
        "  def histogramYolobbox(self):\n",
        "    f = open(self.label_name + '.txt', 'r')\n",
        "    f1 = f.readlines()\n",
        "    new_bbox = []\n",
        "    H, W = self.image.shape[:2]\n",
        "    for x in f1:\n",
        "      bbox = x.strip('\\n').split(' ')\n",
        "      if len(bbox) > 1:\n",
        "        (center_x, center_y, bbox_width, bbox_height) = yoloFormattocv(float(bbox[1]), float(bbox[2]), float(bbox[3]), float(bbox[4]), H, W)\n",
        "        # bbox scale formula\n",
        "        x1 = center_x\n",
        "        y1 = center_y\n",
        "        x2 = bbox_width\n",
        "        y2 = bbox_height\n",
        "      new_bbox.append([bbox[0], x1, y1, x2, y2])\n",
        "\n",
        "    return new_bbox\n",
        "\n",
        "  def histogram_image(self):\n",
        "    ycrcb = cv2.cvtColor(self.image, cv2.COLOR_BGR2YCR_CB)\n",
        "    channels = cv2.split(ycrcb)\n",
        "    cv2.equalizeHist(channels[0], channels[0])\n",
        "    cv2.merge(channels, ycrcb)\n",
        "    cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, self.image)\n",
        "\n",
        "    return self.image\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_frYcCjk_4D"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  image_id_scale = data['image_id'].drop_duplicates()\n",
        "  folder_path = \"/content/gdrive/MyDrive/abnormal_detection/images_jpg\"\n",
        "  augmentation_path = '/content/gdrive/MyDrive/abnormal_detection/augmentation'\n",
        "  iteration = 0\n",
        "  for image_id in tqdm(image_id_scale):\n",
        "    iteration+=1\n",
        "    image_ext = \".jpg\"\n",
        "\n",
        "    image_name = os.path.join(folder_path, \"old_\"+image_id)\n",
        "    label_name = os.path.join(folder_path, \"old_\"+image_id)\n",
        "    target_name = os.path.join(augmentation_path, image_id)\n",
        "\n",
        "    if not (os.path.exists(image_name+\".jpg\") or os.path.exists(image_name+\".txt\")):\n",
        "        continue\n",
        "\n",
        "    im = yoloHistogramEqualization(image_name, label_name, image_ext)\n",
        "    bbox = im.histogramYolobbox()\n",
        "    image = im.histogram_image()\n",
        "    # to write rotateed image to disk\n",
        "    cv2.imwrite(target_name+'_' + \"he\" + '.jpg', image)\n",
        "    file_name = target_name+'_' + \"he\" + '.txt'\n",
        "    \n",
        "    if os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "    # to write the new rotated bboxes to file\n",
        "    for i in bbox:\n",
        "        with open(file_name, 'a') as fout:\n",
        "          fout.writelines(' '.join(map(str, cvFormattoYolo(i, image.shape[0], image.shape[1]))) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0oXHHDYrUAY"
      },
      "source": [
        "class yoloClahe:\n",
        "  def __init__(self, filename, image_ext):\n",
        "    assert os.path.isfile(filename + image_ext)\n",
        "\n",
        "    self.filename = filename\n",
        "    self.image_ext = image_ext\n",
        "\n",
        "    # Read image using cv2\n",
        "    self.image = cv2.imread(self.filename + self.image_ext, 1)\n",
        "\n",
        "  def claheYolobbox(self):\n",
        "    f = open(self.label_name + '.txt', 'r')\n",
        "    f1 = f.readlines()\n",
        "    new_bbox = []\n",
        "    H, W = self.image.shape[:2]\n",
        "    for x in f1:\n",
        "      bbox = x.strip('\\n').split(' ')\n",
        "      if len(bbox) > 1:\n",
        "        (center_x, center_y, bbox_width, bbox_height) = yoloFormattocv(float(bbox[1]), float(bbox[2]), float(bbox[3]), float(bbox[4]), H, W)\n",
        "        # bbox scale formula\n",
        "        x1 = center_x\n",
        "        y1 = center_y\n",
        "        x2 = bbox_width\n",
        "        y2 = bbox_height\n",
        "      new_bbox.append([bbox[0], x1, y1, x2, y2])\n",
        "\n",
        "    return new_bbox\n",
        "\n",
        "  def clahe_image(self):\n",
        "    ycrcb = cv2.cvtColor(self.image, cv2.COLOR_BGR2YCR_CB)\n",
        "    channels = cv2.split(ycrcb)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    clahe.apply(channels[0], channels[0])\n",
        "    cv2.merge(channels, ycrcb)\n",
        "    cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, self.image)\n",
        "\n",
        "    return self.image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd4GM2i0o75N"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  folder_path = '/content/gdrive/MyDrive/abnormal_detection/images'\n",
        "  images_train_dir = '/content/gdrive/MyDrive/abnormal_detection/training_6/images/train'\n",
        "  images_test_dir = '/content/gdrive/MyDrive/abnormal_detection/training_6/images/val'\n",
        "\n",
        "  for image_ids in tqdm(os.listdir(folder_path)):\n",
        "    split_image = image_ids.split('.')\n",
        "    image_id = split_image[0]\n",
        "    image_ext = \".\"+split_image[1]\n",
        "\n",
        "    if image_ext == \".txt\":\n",
        "      continue\n",
        "    \n",
        "    image_name = os.path.join(folder_path, \"{}\".format(image_id))\n",
        "\n",
        "    im = yoloClahe(image_name, image_ext)\n",
        "    image = im.clahe_image()\n",
        "    # to write rotateed image to disk\n",
        "    cv2.imwrite(image_name + '.png', image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTUKfJEuwOcE"
      },
      "source": [
        "  # Read image using cv2\n",
        "  filename = \"/content/0a16dc6491142ff8c7c36f3b3f4ebd02 (1).png\"\n",
        "  image = cv2.imread(filename, 1)\n",
        "  ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCR_CB)\n",
        "  channels = cv2.split(ycrcb)\n",
        "  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "  cl_img = clahe.apply(channels[0], channels[0])\n",
        "\n",
        "  plt.hist(cl_img.flat, bins=100, range=(100, 255))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODmI31sJVVrQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JFB6x7bvscF"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  folder_path = '/content/gdrive/MyDrive/abnormal_detection/testing_dataset'\n",
        "\n",
        "  for image_ids in tqdm(new_image_id):\n",
        "    path_train = os.path.join(folder_path, \"{}.jpg\".format(image_ids))\n",
        "    \n",
        "    image = clahe_image(path_train)\n",
        "    # to write rotateed image to disk\n",
        "    cv2.imwrite(path_train, image)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O24GXQLSLDzG"
      },
      "source": [
        "### **Zoom Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgr5wJjzUaEg"
      },
      "source": [
        "!pip install -U albumentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ4afplPtCig"
      },
      "source": [
        "import albumentations as A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy6JbMlDqpb3"
      },
      "source": [
        "source_path = \"/content/gdrive/MyDrive/abnormal_detection/training_6/images/train\"\n",
        "label_source_path = \"/content/gdrive/MyDrive/abnormal_detection/training_6/labels/train\"\n",
        "\n",
        "for img in tqdm(new_image_id):\n",
        "  img_name = \"old_{}.png\".format(img)\n",
        "  label_name = \"old_{}.txt\".format(img)\n",
        "  img_random_name = \"random_{}.png\".format(img)\n",
        "  label_random_name = \"random_{}.txt\".format(img)\n",
        "  img_path = os.path.join(source_path, img_name)\n",
        "  label_path = os.path.join(label_source_path, label_name)\n",
        "\n",
        "  img_destination = os.path.join(\"/content/gdrive/MyDrive/abnormal_detection/training_6/images/train\", img_random_name)\n",
        "  label_destination = os.path.join(\"/content/gdrive/MyDrive/abnormal_detection/training_6/labels/train\", label_random_name)\n",
        "\n",
        "  transform = A.Compose([\n",
        "    A.IAAPiecewiseAffine(p=0.2),\n",
        "    A.IAASharpen(p=0.2),\n",
        "    A.RandomGamma(gamma_limit=(70, 130), p=0.3),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.75)\n",
        "  ])\n",
        "\n",
        "  image = cv2.imread(img_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  transformed = transform(image=image)\n",
        "  transformed_image = transformed[\"image\"]\n",
        "  cv2.imwrite(img_destination, transformed_image)\n",
        "  shutil.copyfile(label_path, label_destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5jsGbYSnR6h"
      },
      "source": [
        "## **5. Training Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6PcWDzwQPmB"
      },
      "source": [
        "### **Generate train/test for training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khm0gZQHGrhT"
      },
      "source": [
        "new_image_id = []\n",
        "for filename in tqdm(os.listdir(\"/content/gdrive/MyDrive/abnormal_detection/training_7/images/train\")):\n",
        "  file = filename.split('_')\n",
        "  if file[0] == \"old\":\n",
        "    img_split = file[-1].split(\".\")\n",
        "    new_image_id.append(img_split[0])\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Xd_PUbzmut"
      },
      "source": [
        "print(len(new_image_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEN8bU9WXtli"
      },
      "source": [
        "def write_train_list(target_images_dir, target_labels_dir, img_dir, series):\n",
        "  for image_id in series:\n",
        "    images_dir = os.path.join(img_dir, '{}.png'.format(image_id))\n",
        "    labels_dir = os.path.join(img_dir, '{}.txt'.format(image_id))\n",
        "\n",
        "    if os.path.exists(images_dir):    \n",
        "      shutil.copyfile(images_dir, os.path.join(target_images_dir, '{}.png'.format(image_id)))\n",
        "      shutil.copyfile(labels_dir, os.path.join(target_labels_dir, '{}.txt'.format(image_id)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjVS0Ms2QaV_"
      },
      "source": [
        "images_train_dir = '/content/gdrive/MyDrive/abnormal_detection/training_6/images/train'\n",
        "labels_train_dir = '/content/gdrive/MyDrive/abnormal_detection/training_6/labels/train'\n",
        "images_test_dir = '/content/gdrive/MyDrive/abnormal_detection/training_6/images/val'\n",
        "labels_test_dir = '/content/gdrive/MyDrive/abnormal_detection/training_6/labels/val'\n",
        "\n",
        "path_dataset = '/content/gdrive/MyDrive/abnormal_detection/augmentation'\n",
        "# test_images_dir = os.path.join(testing_dir, 'images')\n",
        "# test_labels_dir = os.path.join(testing_dir, 'labels')\n",
        "\n",
        "image_id_series = pd.Series(new_image_id)\n",
        "tr_series, val_series = train_test_split(image_id_series, test_size=0.5, random_state=random_stat)\n",
        "print(\"The # of train set : {}, The # of validation set : {}\".format(tr_series.shape[0], val_series.shape[0]))\n",
        "\n",
        "write_train_list(images_train_dir, labels_train_dir, path_dataset, tr_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olxeE3zHVjM4"
      },
      "source": [
        "files = 0\n",
        "for filename in tqdm(os.listdir(\"/content/gdrive/MyDrive/abnormal_detection/training_6/images/val\")):\n",
        "  files += 1\n",
        "\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf_bhso0ujii"
      },
      "source": [
        "### **Fold Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Corxx2VjunNI"
      },
      "source": [
        "list_image_train = fold_csv[fold_csv[f'fold_{1}'] == 0]['image_id']\n",
        "train_df = new_train_abnormal[new_train_abnormal['image_id'].isin(list_image_train)].reset_index(drop=True)\n",
        "val_df = new_train_abnormal[~new_train_abnormal['image_id'].isin(list_image_train)].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgQnSKys1d2i"
      },
      "source": [
        "tr_series = train_df.image_id.unique()\n",
        "tr_series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tukyv6zXvRCE"
      },
      "source": [
        "images_train_dir = '/content/gdrive/MyDrive/abnormal_detection/training_7/images/train'\n",
        "labels_train_dir = '/content/gdrive/MyDrive/abnormal_detection/training_7/labels/train'\n",
        "images_test_dir = '/content/gdrive/MyDrive/abnormal_detection/training_7/images/val'\n",
        "labels_test_dir = '/content/gdrive/MyDrive/abnormal_detection/training_7/labels/val'\n",
        "\n",
        "path_dataset = '/content/gdrive/MyDrive/abnormal_detection/images'\n",
        "# test_images_dir = os.path.join(testing_dir, 'images')\n",
        "# test_labels_dir = os.path.join(testing_dir, 'labels')\n",
        "\n",
        "# image_id_series = pd.Series(new_image_id)\n",
        "# img_id_series = train_df.image_id.unique() \n",
        "tr_series = train_df.image_id.unique() \n",
        "val_series = val_df.image_id.unique()\n",
        "# tr_series, val_series = train_test_split(img_id_series, test_size=0.7, random_state=random_stat)\n",
        "print(\"The # of train set : {}, The # of validation set : {}\".format(len(tr_series), len(val_series)))\n",
        "\n",
        "write_train_list(images_train_dir, labels_train_dir, path_dataset, tr_series)\n",
        "write_train_list(images_test_dir, labels_test_dir, path_dataset, val_series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SOspzrBwaC"
      },
      "source": [
        "### **Clone Yolov5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCT6i7J3-E3U"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eNfuGgdjaI0"
      },
      "source": [
        "from IPython.display import Image\n",
        "from utils.google_utils import gdrive_download\n",
        "print('torch %s %s' %(torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNzHjvwX6wEK"
      },
      "source": [
        "### **Remove File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BFtnY-5zG4q"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQjZMoW4211z"
      },
      "source": [
        "def delete_scale_image(folder_path, ext_path):\n",
        "  for filename in tqdm(os.listdir(folder_path)):\n",
        "    filepath = os.path.join(folder_path, filename)\n",
        "    scale_name = filename.split(\"_\")\n",
        "    if scale_name[-1] == ext_path:\n",
        "      os.remove(filepath)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "def delete_df(folder_path, label_path, image_ids, imgext, labelext):\n",
        "  i=0\n",
        "  for image_id in tqdm(image_ids):  \n",
        "    imgpath = os.path.join(folder_path, \"{}_{}.png\".format(imgext, image_id))\n",
        "    # labelpath = os.path.join(label_path, \"{}_{}.txt\".format(labelext, image_id))\n",
        "    if os.path.exists(imgpath):\n",
        "      os.remove(imgpath)\n",
        "      # os.remove(labelpath)\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "def convert_jpg_to_png(folder_path):\n",
        "  for filename in tqdm(os.listdir(folder_path)):\n",
        "    filepath = os.path.join(folder_path, filename)\n",
        "    scale_name = filename.split(\".\")\n",
        "    savepath = os.path.join(folder_path, \"{}.png\".format(scale_name[0]))\n",
        "    if scale_name[-1] == \"jpg\":\n",
        "      im1 = Image.open(filepath)\n",
        "      im1.save(savepath)\n",
        "    else:\n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9djU0ILW5_jg"
      },
      "source": [
        "images_train_dir = '/content/gdrive/MyDrive/abnormal_detection/training_7/images/train'\n",
        "labels_train_dir = '/content/gdrive/MyDrive/abnormal_detection/training_7/labels/train'\n",
        "images_test_dir = '/content/gdrive/MyDrive/abnormal_detection/training_7/images/val'\n",
        "labels_test_dir = '/content/gdrive/MyDrive/abnormal_detection/training_7/labels/val'\n",
        "\n",
        "# [20, 90,270, 340, 180]\n",
        "augpath = '/content/gdrive/MyDrive/abnormal_detection/augmentation'\n",
        "\n",
        "delete_scale_image(images_train_dir, \"translation.png\")\n",
        "delete_scale_image(labels_train_dir, \"translation.txt\")\n",
        "# delete_scale_image(augpath, \"jpg\")\n",
        "# delete_scale_image(augpath, \"he.txt\")\n",
        "# convert_jpg_to_png(augpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqFoiqmPJN-j"
      },
      "source": [
        "image_id = data[\"image_id\"].drop_duplicates()\n",
        "print(len(image_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47gxpRNJ2PVP"
      },
      "source": [
        "img_training = \"/content/gdrive/MyDrive/abnormal_detection/training_6/images/train\"\n",
        "img_val = \"/content/gdrive/MyDrive/abnormal_detection/training_6/images/val\"\n",
        "label_training = \"/content/gdrive/MyDrive/abnormal_detection/training_6/labels/train\"\n",
        "label_val = \"/content/gdrive/MyDrive/abnormal_detection/training_6/labels/val\"\n",
        "ext_label = \"old\"\n",
        "ext_image = \"old\"\n",
        "\n",
        "delete_df(img_training, label_training, image_id, ext_image, ext_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbRuxEJ7aMPJ"
      },
      "source": [
        "### **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP0wWtJJQ2Hn"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5GeZvSovlbD"
      },
      "source": [
        "!pip install -U -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udQia6Z-Fytc"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHU5hiOeB070"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnF7W7YFcxQc"
      },
      "source": [
        "!python train.py --img 640 --batch-size 8 --epochs 100 --data '/content/gdrive/MyDrive/yolov5/data/data.yaml' --cfg '/content/gdrive/MyDrive/yolov5/models/yolov5s.yaml' --weights yolov5s.pt --cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3jrCE44E5bC"
      },
      "source": [
        "!python train.py --resume"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWhTbppNHILk"
      },
      "source": [
        "!python detect.py --weights /content/gdrive/MyDrive/yolov5/runs/train/exp14/weights/best.pt --img 640 --conf-thres 0.005 --source /content/gdrive/MyDrive/abnormal_detection/testing_dataset --iou-thres 0.45 --save-txt --save-conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPjtAZ3ZOF_k"
      },
      "source": [
        "### **Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5AApgQxbKM5"
      },
      "source": [
        "size_df = pd.read_csv(\"/content/gdrive/MyDrive/abnormal_detection/test_meta.csv\")\n",
        "size_df.columns = ['image_id', 'h', 'w']\n",
        "\n",
        "sub_df = pd.read_csv(\"/content/gdrive/MyDrive/abnormal_detection/sample_submission.csv\")\n",
        "sub_df = sub_df.merge(size_df, on='image_id', how='left')\n",
        "sub_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AbUntIEehWi"
      },
      "source": [
        "#convert from Yolo_mark to opencv format\n",
        "def yoloFormattocv(x1, y1, x2, y2, H, W):\n",
        "    bbox_width = x2 * W\n",
        "    bbox_height = y2 * H\n",
        "    center_x = x1 * W\n",
        "    center_y = y1 * H\n",
        "    voc = []\n",
        "    voc.append(center_x - (bbox_width / 2))\n",
        "    voc.append(center_y - (bbox_height / 2))\n",
        "    voc.append(center_x + (bbox_width / 2))\n",
        "    voc.append(center_y + (bbox_height / 2))\n",
        "    return [int(v) for v in voc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0bAqoPTUv7z"
      },
      "source": [
        "def predictYolobbox(filename, H, W):\n",
        "    f = open(filename, 'r')\n",
        "    f1 = f.readlines()\n",
        "    new_bbox = []\n",
        "    PredictionString = \"\"\n",
        "    for x in f1:\n",
        "      bbox = x.strip('\\n').split(' ')\n",
        "      if len(bbox) > 1:\n",
        "        (x1, y1, x2, y2) = yoloFormattocv(float(bbox[1]), float(bbox[2]), float(bbox[3]), float(bbox[4]), H, W)\n",
        "\n",
        "        new_bbox.append([bbox[0], bbox[5], x1, y1, x2, y2])\n",
        "    \n",
        "    for bbox in new_bbox:\n",
        "      predict_box = str(bbox[0])+\" \"+str(bbox[1])+\" \"+str(bbox[2])+\" \"+str(bbox[3])+\" \"+str(bbox[4])+\" \"+str(bbox[5])+\" \"\n",
        "      PredictionString = PredictionString + predict_box\n",
        "\n",
        "    return PredictionString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh9uCRd2X-ug"
      },
      "source": [
        "def populationYolobbox(image_id, filename, H, W):\n",
        "    split_filename = image_id.split(\".\")\n",
        "    f = open(filename, 'r')\n",
        "    f1 = f.readlines()\n",
        "    new_bbox = []\n",
        "    population_data = []\n",
        "    for x in f1:\n",
        "      bbox = x.strip('\\n').split(' ')\n",
        "      if len(bbox) > 1:\n",
        "        # (x1, y1, x2, y2) = yoloFormattocv(float(bbox[1]), float(bbox[2]), float(bbox[3]), float(bbox[4]), H, W)\n",
        "        if float(bbox[1]) > 0.2:\n",
        "          new_bbox.append([split_filename[0], float(bbox[0]), float(bbox[1])])\n",
        "    \n",
        "    \n",
        "    # for bbox in new_bbox:\n",
        "    #   predict_box = split_filename[0]+\" \"+str(bbox[0])+\" \"+str(bbox[1])\n",
        "    #   population_data.append(predict_box)\n",
        "\n",
        "    return new_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYkfVgr_4xgZ"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/abnormal_detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEpnG6Df9-Aq"
      },
      "source": [
        "import random\n",
        "import statistics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD0xqMQOoEjF"
      },
      "source": [
        "label_path = \"/content/gdrive/MyDrive/yolov5/runs/detect/exp14/labels\"\n",
        "submission = []\n",
        "for _, row in tqdm(sub_df.iterrows()):\n",
        "  filename = os.path.join(label_path, \"{}.txt\".format(row['image_id']))\n",
        "  predictionString = [[row['image_id'], 14, 0]]\n",
        "  if path.exists(filename):\n",
        "    predictionString = populationYolobbox(row['image_id'], filename, row['h'], row['w'])\n",
        "  for population in predictionString:\n",
        "    submission.append(population) \n",
        "\n",
        "# submission_df = pd.DataFrame(submission, columns=['image_id', 'PredictionString'])\n",
        "submission_df = pd.DataFrame(submission, columns=['image_id', 'id_class', 'prediction'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZAU_AI7WZR"
      },
      "source": [
        "def Visualize_class(df, feature, title):\n",
        "  num_image = df[feature].value_counts().rename_axis(feature).reset_index(name=\"num_image\")\n",
        "  fig = px.bar(num_image[::-1], x='num_image', y=feature, orientation='h', color='num_image')\n",
        "  fig.update_layout(\n",
        "      title={\n",
        "          'text' : title,\n",
        "          'y' : 0.95,\n",
        "          'x' : 0.5,\n",
        "          'xanchor' : 'center',\n",
        "          'yanchor' : 'top'})\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrdaQx9U7XVb"
      },
      "source": [
        "Visualize_class(submission_df, feature=\"id_class\", title=\"Types of abnormal labels\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXZrWi1MlOng"
      },
      "source": [
        "submission_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89Xf_lTvxTxu"
      },
      "source": [
        "column_name = \"prediction\"\n",
        "sum_sub = list(submission_df[column_name])\n",
        "length_population = len(sum_sub)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyLQvNOFHn5O"
      },
      "source": [
        "print(length_population)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXY1AhUZzsZe"
      },
      "source": [
        "jumlah_population = math.fsum(list(sum_sub))\n",
        "rata2_population = jumlah_population/length_population\n",
        "print(jumlah_population)\n",
        "print(rata2_population)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAlBTjs46us_"
      },
      "source": [
        "sample_random = random.sample(sum_sub, 500)\n",
        "jumlah_sample = math.fsum(sample_random)\n",
        "length_sample = len(sample_random)\n",
        "rata2_sample = jumlah_sample/length_sample\n",
        "stdev_sample = statistics.stdev(sample_random)\n",
        "print(jumlah_sample)\n",
        "print(rata2_sample)\n",
        "print(stdev_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRKl-7U4zvaP"
      },
      "source": [
        "stdev_x = 0\n",
        "for x in sample_random:\n",
        "  stdev_x = stdev_x + (x - rata2_sample)**2\n",
        "print(stdev_x)\n",
        "stdev_sample = math.sqrt(stdev_x/499)\n",
        "print(stdev_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDMLUCEJCdu2"
      },
      "source": [
        "koef_t = (rata2_sample-rata2_population)/(stdev_sample/math.sqrt(length_sample))\n",
        "print(koef_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX9yrJQlxhsa"
      },
      "source": [
        "submission_df.to_csv('uji_hipotesis.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EONl9clhThcg"
      },
      "source": [
        "## **Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x918QbkhI7ld"
      },
      "source": [
        "model = torch.hub.load('/content/yolov5', 'custom', path='/content/gdrive/MyDrive/abnormal_detection/hasil_training/5-6-2021_ep43_512_8_yolov5x_preprocessing/train/exp9/weights/best.pt', source='local')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnr48bIp17mD"
      },
      "source": [
        "img = cv2.imread('/content/gdrive/MyDrive/yolov5/runs/detect/exp/008bdde2af2462e86fd373a445d0f4cd.jpg')\n",
        "results = model(img) \n",
        "\n",
        "results.print()\n",
        "\n",
        "results.xyxy[0]\n",
        "bbox = results.pandas().xyxy[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n5knmvPLB6T"
      },
      "source": [
        "print(bbox['xmin'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbUMQb9OMTj7"
      },
      "source": [
        "pneumonia = pd.read_csv(\"/content/gdrive/MyDrive/Pneumonia_dataset/stage_2_train_labels.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGEDFTxq3E-A"
      },
      "source": [
        "pneumonia.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1DzsffH3cUJ"
      },
      "source": [
        "testing = pneumonia[pneumonia.Target == 1].drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hln4Dvfi3lcd"
      },
      "source": [
        "testing.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGWuBRrD3nYH"
      },
      "source": [
        "len(testing)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}